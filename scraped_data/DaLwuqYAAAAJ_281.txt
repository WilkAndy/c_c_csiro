{"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "A Comparison of Saliency Methods for Deep Learning Explainability", "pub_year": 2021, "citation": "2021 Digital Image Computing: Techniques and Applications (DICTA), 01-08, 2021", "author": "Salamata Konate and L\u00e9o Lebrat and Rodrigo Santa Cruz and Elliot Smith and Andrew Bradley and Clinton Fookes and Olivier Salvado", "conference": "2021 Digital Image Computing: Techniques and Applications (DICTA)", "pages": "01-08", "publisher": "IEEE", "abstract": "Saliency methods are widely used to visually explain \u201cblack-box\u201d deep learning model outputs to humans. These methods produce meaningful maps which aim to identify the salient part of an image responsible for, and so best explain, a Convolutional Neural Network (CNN) decision. In this paper, we consider the case of a classifier and the role of the two main categories of saliency methods: backpropagation and attribution. The first method is based on the gradient of the output with respect to the network parameters, while the second tests how local image perturbations affect the output. In this paper, we compare the Gradient method, Grad-CAM, Extremal perturbation, and DEEPCOVER, and highlight the complexity in determining which method provides the best explanation of a CNN's decision."}, "filled": true, "author_pub_id": "DaLwuqYAAAAJ:5kgRglCLipYC", "num_citations": 1, "citedby_url": "/scholar?hl=en&cites=646638620628813542", "cites_id": ["646638620628813542"], "pub_url": "https://ieeexplore.ieee.org/abstract/document/9647419/", "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:5gahQGZS-QgJ:scholar.google.com/", "cites_per_year": {"2023": 1}}