{"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Man-in-the-middle attacks against machine learning classifiers via malicious generative models", "pub_year": 2020, "citation": "IEEE Transactions on Dependable and Secure Computing 18 (5), 2074-2087, 2020", "author": "Derui Wang and Chaoran Li and Sheng Wen and Surya Nepal and Yang Xiang", "journal": "IEEE Transactions on Dependable and Secure Computing", "volume": "18", "number": "5", "pages": "2074-2087", "publisher": "IEEE", "abstract": "Deep Neural Networks (DNNs) are vulnerable to deliberately crafted adversarial examples. In the past few years, many efforts have been spent on exploring query-optimisation attacks to find adversarial examples of either black-box or white-box DNN models, as well as the defending countermeasures against those attacks. In this article, we explore vulnerabilities of DNN models under the umbrella of Man-in-the-Middle (MitM) attacks, which have not been investigated before. From the perspective of an MitM adversary, the aforementioned adversarial example attacks are not viable anymore. First, such attacks must acquire the outputs from the models multiple times before actually launching attacks, which is difficult for the MitM adversary in practice. Second, such attacks are one-off and cannot be directly generalised onto new data examples, which decreases the rate of return for the attacker. In contrast, using \u2026"}, "filled": true, "author_pub_id": "ge6zzwIAAAAJ:FiDNX6EVdGUC", "num_citations": 23, "citedby_url": "/scholar?hl=en&cites=8266117441442936884", "cites_id": ["8266117441442936884"], "pub_url": "https://ieeexplore.ieee.org/abstract/document/9183938/", "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:NPjdaAQmt3IJ:scholar.google.com/", "cites_per_year": {"2020": 1, "2021": 4, "2022": 11, "2023": 7}}