{"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "AgrEvader: Poisoning Membership Inference against Byzantine-robust Federated Learning", "pub_year": 2023, "citation": "Proceedings of the ACM Web Conference 2023, 2371-2382, 2023", "author": "Yanjun Zhang and Guangdong Bai and Mahawaga Arachchige Pathum Chamikara and Mengyao Ma and Liyue Shen and Jingwei Wang and Surya Nepal and Minhui Xue and Long Wang and Joseph Liu", "pages": "2371-2382", "abstract": "The Poisoning Membership Inference Attack (PMIA) is a newly emerging privacy attack that poses a significant threat to federated learning (FL). An adversary conducts data poisoning (i.e., performing adversarial manipulations on training examples) to extract membership information by exploiting the changes in loss resulting from data poisoning. The PMIA significantly exacerbates the traditional poisoning attack that is primarily focused on model corruption. However, there has been a lack of a comprehensive systematic study that thoroughly investigates this topic. In this work, we conduct a benchmark evaluation to assess the performance of PMIA against the Byzantine-robust FL setting that is specifically designed to mitigate poisoning attacks. We find that all existing coordinate-wise averaging mechanisms fail to defend against the PMIA, while the detect-then-drop strategy was proven to be effective in most cases \u2026"}, "filled": true, "author_pub_id": "ge6zzwIAAAAJ:3lUAU8Oskd0C", "num_citations": 0, "pub_url": "https://dl.acm.org/doi/abs/10.1145/3543507.3583542", "cites_per_year": {}}