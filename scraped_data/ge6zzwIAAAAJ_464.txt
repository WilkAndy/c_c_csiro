{"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Explainable machine learning in cybersecurity: A survey", "pub_year": 2022, "citation": "International Journal of Intelligent Systems 37 (12), 12305-12334, 2022", "author": "Feixue Yan and Sheng Wen and Surya Nepal and Cecile Paris and Yang Xiang", "journal": "International Journal of Intelligent Systems", "volume": "37", "number": "12", "pages": "12305-12334", "abstract": "Machine learning (ML) techniques are increasingly important in cybersecurity, as they can quickly analyse and identify different types of threats from millions of events. In spite of the increasing number of possible applications of ML, successful adoption of ML models in cybersecurity still highly relies on the explainability of those models that are used for making predictions. Explanations that support ML model outputs are crucial in cybersecurity\u2010oriented ML applications because people need to get more information from the model than just binary output for analysis. The explainable models help ML developers solve the \u201ctrust\u201d problem for a security application prediction in a faithful way: validating model behaviours, diagnosing misclassifications and sometimes automatically patching errors in the target models. Therefore, explainable ML for cybersecurity has become a necessary and important research branch. In \u2026"}, "filled": true, "author_pub_id": "ge6zzwIAAAAJ:9shLKfS_uJEC", "num_citations": 0, "pub_url": "https://onlinelibrary.wiley.com/doi/abs/10.1002/int.23088", "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:Tw1reJNSBlwJ:scholar.google.com/", "cites_per_year": {}}